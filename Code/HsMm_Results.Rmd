---
title: "Results"
author: "Laura Symul"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  bookdown::html_document2: 
    theme: flatly
    highlight: haddock
    toc: yes
    toc_float: true
    toc_depth: 5
    number_sections: true
    fig_caption: true
---


```{r results setup, include = FALSE, eval = TRUE, cache = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
source("Scripts/00_setup.R")
```



# Summary of the results {#results-summary}


This document presents the `HiddenSemiMarkov` `R` package which provides an environment to simulate and decode sequences with semi-Markov models and to fit such models to observation sequences.

The development of this package was needed because other existing `R` packages for hidden semi-Markov models did not meet the requirements necessary to decode sparse multi-variate time-series with variables of different types and with state-dependent and variable-dependent missing data.

In section \@ref(pkg-descr), we describe the package usage and justify the implemented methods. In particular, we describe how the joint emission probabilities are initialized and later updated as the model is fitted to observations. They are initialzied assuming independence of the variables given the states. However, once the model is fitted to sequences of variables that have within-state correlations, then these correlations are learned as the joint emission probabilities are updated. In particular, section \@ref(pkg-fit-example) demonstrates the ability of our framework to learn these dependencies as the updated joint-probabilities exhibit a strong within-state correlation between variables. \lsy{Actually, the method I describe for the joint probabilities (initialization and fitting) is something I would love feedback on as it is the crux of the problem. I've tried different methods, but so far, this one is the most stable, although a little slow - I have some ideas on how to make it faster but I'd really appreciate talking about it before I try.}

In section \@ref(perf-sim-section), we evaluate the performances of the model to recover simulated state sequences when challenged with an increasing level of missing data. The results show that our framework/model is able to recover at least 75% of the ground-truth when up to 60% of the data is missing (figures \@ref(fig:perf-sim-increasing-missingness-viz-all-m)). The confusion matrices (same section) show that the loss in accuracy is mostly due to a lack of precision on the timing of state transitions rather than an inability to recognize states. In other words, the model is unsure when a state exactly starts or ends, but it is still able to recognize which states were the most likely.

Finally, in section \@ref(real-data), we evaluate the performances of the model to decode real-life time-series, _i.e._ time-series that real users of the Kindara app have tracked. The performances were evaluated on the time-series of 114 users and we manually labelled about 15% of these time-series to establish some ground-truth. The results show that xxxx \lsy{@Susan: I know that the results don't look impressive yet, but I don't think I'm taking the right approach to decode real time-series. In summary, right now, I group users by their tracking behavior, but I think I should instead decode all time-series with the simplest model, then extract the parts of the time-series that have richer data (users are not consistent in their tracking behavior) and decode these parts with more complex models. I did some quick visual checks of the current decodings and that seems to validate this idea... I just ran out of time before sending you this document.}




